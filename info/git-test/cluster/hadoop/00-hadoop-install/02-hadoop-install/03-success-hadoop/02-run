1. start
-----------------------------------hdfs----------------------------------------------------
[hadoop@node1 tmp]$ hdfs namenode -format
$HADOOP_HOME/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start namenode
$HADOOP_HOME/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs start datanode
----------------------------------yarn-----------------------------------------------------
$HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start resourcemanager
$HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR start nodemanager



2. stop
-----------------------------------hdfs----------------------------------------------------
$HADOOP_HOME/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop namenode
$HADOOP_HOME/sbin/hadoop-daemon.sh --config $HADOOP_CONF_DIR --script hdfs stop datanode
-----------------------------------yarn----------------------------------------------------
$HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop resourcemanager
$HADOOP_YARN_HOME/sbin/yarn-daemon.sh --config $HADOOP_CONF_DIR stop nodemanager

3. hdfs
[hadoop@node1 hadoop]$ hadoop fs -ls /
[hadoop@node1 hadoop]$ hadoop fs -mkdir /testdir2


4. mapreduce
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.0.3-alpha.jar pi 2 1000


============================================================================================
Note:
	it is also successful when using the tar ball from apache website.
