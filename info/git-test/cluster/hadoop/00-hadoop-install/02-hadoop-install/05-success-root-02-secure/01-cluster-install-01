================================================================================
1. create hadoop group, add hdfs/yarn/mapred with the same group 'hadoop', set passwd and ssh no-passwd
	hdfs:hadoop	NameNode, Secondary NameNode, Checkpoint Node, Backup Node, DataNode
	yarn:hadoop	ResourceManager, NodeManager
	mapred:hadoop	MapReduce JobHistory Server
================================================================================	
2. copy compiled hadoop-2.0.3-alpha.tar.gz to /usr/ on each node, untar
[root@node1 target]# scp hadoop-2.0.3-alpha.tar.gz /usr/
[root@node1 target]# scp hadoop-2.0.3-alpha.tar.gz node2:/usr/
[root@node1 target]# scp hadoop-2.0.3-alpha.tar.gz node3:/usr/
[root@node1 target]# scp hadoop-2.0.3-alpha.tar.gz node4:/usr/
================================================================================
3. vi /etc/profile  ~/.bashrc
--------------------------------------------------------------------------------
#for java
export JAVA_HOME=/usr/local/jdk1.6.0_21
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$CLASSPATH


#hadoop_home
export HADOOP_HOME=/usr/hadoop-2.0.3-alpha
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_YARN_HOME=$HADOOP_HOME
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH

export HADOOP_CONF_DIR=/etc/hadoop/conf
export YARN_CONF_DIR=$HADOOP_CONF_DIR
--------------
Note: check 
[root@node1 ~]# echo $HADOOP_HOME
/usr/hadoop-2.0.3-alpha
================================================================================
4. config  /etc/hadoop/conf
[root@node1 hadoop]# pwd
/usr/hadoop-2.0.3-alpha/etc/hadoop
[root@node1 hadoop]# cp * /etc/hadoop/conf/
---------------------------------
core-site.xml
---------------------------------
<property>
        <name>fs.defaultFS</name>
        <value>hdfs://10.37.7.101:9000</value>
</property>
---------------------------------
hdfs-site.xml
---------------------------------
<property>
	<name>dfs.replication</name>
	<value>1</value>
</property>

<property>
	<name>dfs.namenode.name.dir</name>
	<value>file:///tmp/hadoop-hdfs/data/hdfs/namenode</value>
</property>

<property>
	<name>dfs.datanode.data.dir</name>
	<value>file:///tmp/hadoop-hdfs/data/hdfs/datanode</value>
</property>

<property>
	<name>dfs.block.size</name>
	<value>134217728</value>
</property>
---------------------------------
yarn-site.xml
---------------------------------
<property>
    <name>yarn.resourcemanager.scheduler.class</name>
    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
</property>

<property>
	<name>yarn.nodemanager.aux-services</name>
	<value>mapreduce.shuffle</value>
</property>

<property>
	<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
	<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>

<property>
	<name>yarn.resourcemanager.resource-tracker.address</name>
	<value>10.37.7.101:8025</value>
</property>

<property>
	<name>yarn.resourcemanager.scheduler.address</name>
	<value>10.37.7.101:8030</value>
</property>

<property>
	<name>yarn.resourcemanager.address</name>
	<value>10.37.7.101:8040</value>
</property>

---------------------------------
mapred-site.xml
---------------------------------
<property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
</property>
================================================================================
5. slaves   /etc/hadoop/conf
#vi slaves
10.37.7.102
10.37.7.103
10.37.7.104

node2
node3
node4
================================================================================
6. java_home 	/etc/hadoop/conf
#vi hadoop-env.sh
export JAVA_HOME=/usr/local/jdk1.6.0_21

export HADOOP_LOG_DIR=/tmp/hadoop_log_dir
================================================================================
7. [root@node1 hadoop]# scp -r /etc/hadoop/conf node4:/etc/hadoop/