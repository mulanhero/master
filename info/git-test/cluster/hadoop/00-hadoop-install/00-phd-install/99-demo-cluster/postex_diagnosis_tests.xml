<?xml version="1.0" encoding="ISO-8859-1"?>
<!--
**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
**  
 -->
<!-- This is a diagnostic test configuration file. Diagnostic test driver 
     reads this file to get the list of tests and their configuration information
     
   Title            : Provides brief description of the test
   ClassName        : Provides the fully qualified java class name that implements the test condition
   Description      : Provides detailed information about the test describing how it checks for a specific
                        performance problem.
   SuccessThreshold : (value between [0..1])        
                    : Evaluation of a diagnostic test returns its level of impact on the job
                      performance. If impact value [between 0..1] is equal or greater than the 
                      success threshold, means rule has detected the problem (TEST POSITIVE) else 
                      rule has passed the test (TEST NEGATIVE). The impact level is calculated and 
                      returned by each test's evaluate method. For tests that are boolean in nature 
                      the impact level is either 0 or 1 and success threshold should be 1.
   Importance       : Indicates relative importance of this diagnostic test among the set of
                      diagnostic rules defined in this file. Three declarative values that
                      can be assigned are High, Medium or Low
   Prescription     : This is an optional element to store the advice to be included in the report upon test failure
                      This is overwritten in the report by any advice/prescription text returned by getPrescription method of 
                      DiagnosticTest.  
   InputElement     : Input element is made available to the diagnostic test for it to interpret and accept 
                      any parameters specific to the test. These test specific parameters are used to configure 
                      the tests without changing the java code.
-->
<PostExPerformanceDiagnosisTests>

    <DiagnosticTest>
        <Title><![CDATA[Balanced Reduce Partitioning]]></Title>
        <ClassName><![CDATA[org.apache.hadoop.vaidya.postexdiagnosis.tests.BalancedReducePartitioning]]></ClassName>
        <Description><![CDATA[This rule tests as to how well the input to reduce tasks is balanced]]></Description>
        <Importance><![CDATA[High]]></Importance>
        <SuccessThreshold><![CDATA[0.40]]></SuccessThreshold>
        <Prescription><![CDATA[advice]]></Prescription>
        <InputElement>
            <PercentReduceRecords><![CDATA[85]]></PercentReduceRecords>
        </InputElement>
    </DiagnosticTest>

    <DiagnosticTest>
        <Title><![CDATA[Impact of Map tasks Re-Execution]]></Title>
        <ClassName><![CDATA[org.apache.hadoop.vaidya.postexdiagnosis.tests.MapsReExecutionImpact]]></ClassName>
        <Description>
            <![CDATA[This test rule fails, if percentage of map tasks failed is greater than FailedTasksPercentThreshold OR percent of total launched map tasks which includes both killed and failed map tasks is greater than LaunchedTasksPercentThreshold]]></Description>
        <Importance><![CDATA[High]]></Importance>
        <SuccessThreshold><![CDATA[0.40]]></SuccessThreshold>
        <Prescription><![CDATA[default advice]]></Prescription>
        <InputElement>
            <FailedTasksPercentThreshold>20</FailedTasksPercentThreshold>
            <LaunchedTasksPercentThreshold>50</LaunchedTasksPercentThreshold>
        </InputElement>
    </DiagnosticTest>

    <DiagnosticTest>
        <Title><![CDATA[Impact of Reduce tasks Re-Execution]]></Title>
        <ClassName><![CDATA[org.apache.hadoop.vaidya.postexdiagnosis.tests.ReducesReExecutionImpact]]></ClassName>
        <Description>
            <![CDATA[This test rule fails, if percentage of reduce tasks failed is greater than FailedTasksPercentThreshold OR percent of total launched reduce tasks which includes both killed and failed tasks is greater than LaunchedTasksPercentThreshold]]></Description>
        <Importance><![CDATA[High]]></Importance>
        <SuccessThreshold><![CDATA[0.40]]></SuccessThreshold>
        <Prescription><![CDATA[default advice]]></Prescription>
        <InputElement>
            <FailedTasksPercentThreshold>20</FailedTasksPercentThreshold>
            <LaunchedTasksPercentThreshold>50</LaunchedTasksPercentThreshold>
        </InputElement>
    </DiagnosticTest>

    <DiagnosticTest>
        <Title><![CDATA[Map side disk spill]]></Title>
        <ClassName><![CDATA[org.apache.hadoop.vaidya.postexdiagnosis.tests.MapSideDiskSpill]]></ClassName>
        <Description>
            <![CDATA[This test rule checks if Map tasks are spilling the data on to the local disk during the map side sorting due to insufficient sort buffer size. The impact is calculated as ratio between local bytes written to map output bytes. Impact is normalized using NormalizationFactor and any value greater than or equal to NormalizationFactor is treated as maximum impact]]></Description>
        <Importance><![CDATA[Low]]></Importance>
        <SuccessThreshold><![CDATA[0.50]]></SuccessThreshold>
        <Prescription><![CDATA[default advice]]></Prescription>
        <InputElement>
            <NormalizationFactor>3.0</NormalizationFactor>
        </InputElement>
    </DiagnosticTest>

    <DiagnosticTest>
        <Title><![CDATA[Use of Combiner]]></Title>
        <ClassName><![CDATA[org.apache.hadoop.vaidya.postexdiagnosis.tests.UseOfCombiner]]></ClassName>
        <Description>
            <![CDATA[This test rule checks if there is a potential in usage of the combiner to reduce intermediate data and thus fails the rule, if combiner is not in use. To check for the potential in usage of the combiner, this rule checks, if the ratio of reduce input to output records is more than the configurable threshold (ReduceRecordsRatioThreshold)]]></Description>
        <Importance><![CDATA[Medium]]></Importance>
        <SuccessThreshold><![CDATA[1.0]]></SuccessThreshold>
        <Prescription><![CDATA[default advice]]></Prescription>
        <InputElement>
            <ReduceRecordsRatioThreshold>2.0</ReduceRecordsRatioThreshold>
        </InputElement>
    </DiagnosticTest>

    <DiagnosticTest>
        <Title><![CDATA[Inefficient Use of Combiner]]></Title>
        <ClassName><![CDATA[org.apache.hadoop.vaidya.postexdiagnosis.tests.InefficientUseOfCombiner]]></ClassName>
        <Description>
            <![CDATA[This test rule checks if combiner is in use but not efficient enough. It checks if the ratio of combiner input to output records is less than or equal to a specified threshold i.e. CombineRecordsRatioThreshold]]></Description>
        <Importance><![CDATA[Medium]]></Importance>
        <SuccessThreshold><![CDATA[1.0]]></SuccessThreshold>
        <Prescription><![CDATA[default advice]]></Prescription>
        <InputElement>
            <CombineRecordsRatioThreshold>1.10</CombineRecordsRatioThreshold>
        </InputElement>
    </DiagnosticTest>

    <DiagnosticTest>
        <Title><![CDATA[MinSplitSize vs. HDFS block-size]]></Title>
        <ClassName><![CDATA[org.apache.hadoop.vaidya.postexdiagnosis.tests.MinSplitSize]]></ClassName>
        <Description>
            <![CDATA[This test rule checks if mapreduce.input.fileinputformat.split.minsize parameter is greater than default HDFS block size. In which case it is good to increase the default HDFS block size for input data files]]></Description>
        <Importance><![CDATA[Medium]]></Importance>
        <SuccessThreshold><![CDATA[1.0]]></SuccessThreshold>
        <Prescription><![CDATA[default advice]]></Prescription>
    </DiagnosticTest>

    <DiagnosticTest>
        <Title><![CDATA[Intermediate Data Compression]]></Title>
        <ClassName><![CDATA[org.apache.hadoop.vaidya.postexdiagnosis.tests.IntermediateDataCompression]]></ClassName>
        <Description>
            <![CDATA[This test rule fails, if intermediate data compression is not enabled for MAP-REDUCE jobs or it is enabled but percent compression is less than a specified PercentCompressionThreshold. In later case job may benefit without using intermediate compression]]></Description>
        <Importance><![CDATA[Medium]]></Importance>
        <SuccessThreshold><![CDATA[1.0]]></SuccessThreshold>
        <Prescription><![CDATA[default advice]]></Prescription>
        <InputElement>
            <PercentCompressionThreshold>20</PercentCompressionThreshold>
        </InputElement>
    </DiagnosticTest>

    <DiagnosticTest>
        <Title><![CDATA[Output Data Compression]]></Title>
        <ClassName><![CDATA[org.apache.hadoop.vaidya.postexdiagnosis.tests.OutputDataCompression]]></ClassName>
        <Description>
            <![CDATA[This test rule checks if output data is Bzip2 compressed. Bzip2 compressed data can be split for the next processing stage. This check will make user aware of Bzip2 compression]]></Description>
        <Importance><![CDATA[Medium]]></Importance>
        <SuccessThreshold><![CDATA[1.0]]></SuccessThreshold>
        <Prescription><![CDATA[default advice]]></Prescription>
    </DiagnosticTest>

    <DiagnosticTest>
        <Title><![CDATA[Map Data Locality]]></Title>
        <ClassName><![CDATA[org.apache.hadoop.vaidya.postexdiagnosis.tests.MapDataLocality]]></ClassName>
        <Description>
            <![CDATA[This test rule checks if percentage of Data local maps is less than PercentDataLocalityThreshold, in which case it fails the test rule indicating poor data locality for the job.]]></Description>
        <Importance><![CDATA[Medium]]></Importance>
        <SuccessThreshold><![CDATA[1.0]]></SuccessThreshold>
        <Prescription><![CDATA[default advice]]></Prescription>
        <InputElement>
            <PercentDataLocalityThreshold>50</PercentDataLocalityThreshold>
        </InputElement>
    </DiagnosticTest>

    <DiagnosticTest>
        <Title><![CDATA[Map Execution Granularity]]></Title>
        <ClassName><![CDATA[org.apache.hadoop.vaidya.postexdiagnosis.tests.MapExecutionGranularity]]></ClassName>
        <Description>
            <![CDATA[This test rule checks, if the absolute median execution time of map tasks is less than recommended map execution time granularity threshold. Typically map execution granularity should be low for better load balancing and re-execution of failed tasks]]></Description>
        <Importance><![CDATA[Medium]]></Importance>
        <SuccessThreshold><![CDATA[1.0]]></SuccessThreshold>
        <Prescription><![CDATA[default advice]]></Prescription>
        <InputElement>
            <MapExecutionGranularityThreshodInSeconds>900</MapExecutionGranularityThreshodInSeconds>
        </InputElement>
    </DiagnosticTest>

    <DiagnosticTest>
        <Title><![CDATA[Long Running Maps]]></Title>
        <ClassName><![CDATA[org.apache.hadoop.vaidya.postexdiagnosis.tests.LongRunningMaps]]></ClassName>
        <Description>
            <![CDATA[This test rule checks for long running maps. It fails, if one or more maps are running N times longer than average map execution time. N is specified as an input to this test rule. Also this test rule calculates the average over first M percet of map tasks, arranged in their ascending order of execution time. This would exclude last (100-M) percent of possibly long running map tasks from the average execution time. This rule provides hint for using speculative execution.]]></Description>
        <Importance><![CDATA[Medium]]></Importance>
        <SuccessThreshold><![CDATA[1.0]]></SuccessThreshold>
        <Prescription><![CDATA[default advice]]></Prescription>
        <InputElement>
            <N>10</N>
            <AverageOverPercent>85</AverageOverPercent>
        </InputElement>
    </DiagnosticTest>

    <DiagnosticTest>
        <Title><![CDATA[Long Running Reduces]]></Title>
        <ClassName><![CDATA[org.apache.hadoop.vaidya.postexdiagnosis.tests.LongRunningReduces]]></ClassName>
        <Description>
            <![CDATA[This test rule checks for exceptionally long running reduce tasks. It fails, if one or more of them are running N times longer than average reduce execution time. N is specified as an input to this test rule. Also this test rule calculates the average over first M percet of reduce tasks, arranged in their ascending order of execution time. This would exclude last (100-M) percent of possibly long running reduce tasks from the average. This rule provides a hint for using speculative execution]]></Description>
        <Importance><![CDATA[Medium]]></Importance>
        <SuccessThreshold><![CDATA[1.0]]></SuccessThreshold>
        <Prescription><![CDATA[default advice]]></Prescription>
        <InputElement>
            <N>10</N>
            <AverageOverPercent>85</AverageOverPercent>
        </InputElement>
    </DiagnosticTest>

    <DiagnosticTest>
        <Title><![CDATA[Single Reducer Jobs]]></Title>
        <ClassName><![CDATA[org.apache.hadoop.vaidya.postexdiagnosis.tests.SingleReducerJob]]></ClassName>
        <Description>
            <![CDATA[This test rule fails, if job has a single reduce task. Having single reduce task is not necessarily a problem, although failure of this rule highlights the fact where user may have forgotten to explicitly set the number of reduce tasks and hence may be running the job slower with default single reduce task]]></Description>
        <Importance><![CDATA[Medium]]></Importance>
        <SuccessThreshold><![CDATA[1.0]]></SuccessThreshold>
        <Prescription><![CDATA[default advice]]></Prescription>
    </DiagnosticTest>

    <DiagnosticTest>
        <Title><![CDATA[Use of Ubertask]]></Title>
        <ClassName><![CDATA[org.apache.hadoop.vaidya.postexdiagnosis.tests.UseOfUbertask]]></ClassName>
        <Description><![CDATA[This rule checks whether your job is suitable for ubertask optimization]]></Description>
        <Importance><![CDATA[Medium]]></Importance>
        <SuccessThreshold><![CDATA[1.0]]></SuccessThreshold>
        <Prescription><![CDATA[advice]]></Prescription>
        <InputElement>
            <MaxUberTaskMaps><![CDATA[9]]></MaxUberTaskMaps>
            <MaxUberTaskReduces><![CDATA[1]]></MaxUberTaskReduces>
            <MaxUberTaskBytes><![CDATA[67108864]]></MaxUberTaskBytes>
        </InputElement>
    </DiagnosticTest>

</PostExPerformanceDiagnosisTests>
