----------------------------------------------------------------
1. 
----------------------------------------------------------------
[root@gphd-vm211 soft]# cat /usr/local/hamster/testsuite-bin/lengths.file 
4096
16384
65536
262144
1048576
4194304
----------------------------------------------------------------
2. 
----------------------------------------------------------------
[root@gphd-vm211 soft]# mpirun -np 4 /usr/local/hamster/testsuite-bin/imb/IMB-MPI1 pingping alltoall reduce_scatter -msglen /usr/local/hamster/testsuite-bin/lengths.file
[gphd-vm211:11536] STATISTIC: orte timecost_daemon_launch = 0.000011
[gphd-vm211:11536] STATISTIC: orte timecost_proc_launch = 0.029334
[gphd-vm211:11536] STATISTIC: hamster proc_launch_end.tv_sec = 1387288505, proc_launch_end.tv_usec = 559985
 benchmarks to run pingping alltoall reduce_scatter 
#---------------------------------------------------
#    Intel (R) MPI Benchmark Suite V3.2.4, MPI-1 part    
#---------------------------------------------------
# Date                  : Tue Dec 17 21:55:05 2013
# Machine               : x86_64
# System                : Linux
# Release               : 2.6.32-358.el6.x86_64
# Version               : #1 SMP Fri Feb 22 00:31:26 UTC 2013
# MPI Version           : 2.1
# MPI Thread Environment: 

# New default behavior from Version 3.2 on:

# the number of iterations per message size is cut down 
# dynamically when a certain run time (per message size sample) 
# is expected to be exceeded. Time limit is defined by variable 
# "SECS_PER_SAMPLE" (=> IMB_settings.h) 
# or through the flag => -time 
  


# Calling sequence was: 

# /usr/local/hamster/testsuite-bin/imb/IMB-MPI1 pingping alltoall reduce_scatter -msglen /usr/local/hamster/testsuite-bin/lengths.file

# Message lengths were user defined
#
# MPI_Datatype                   :   MPI_BYTE 
# MPI_Datatype for reductions    :   MPI_FLOAT
# MPI_Op                         :   MPI_SUM  
#
#

# List of Benchmarks to run:

# PingPing
# Alltoall
# Reduce_scatter

#---------------------------------------------------
# Benchmarking PingPing 
# #processes = 2 
# ( 2 additional processes waiting in MPI_Barrier)
#---------------------------------------------------
       #bytes #repetitions      t[usec]   Mbytes/sec
         4096         1000        22.97       170.06
        16384         1000        31.50       495.97
        65536          640        66.74       936.42
       262144          160       235.48      1061.68
      1048576           40       889.70      1123.97
      4194304           10      4233.10       944.93

#----------------------------------------------------------------
# Benchmarking Alltoall 
# #processes = 2 
# ( 2 additional processes waiting in MPI_Barrier)
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
         4096         1000        27.37        27.38        27.37
        16384         1000        23.10        23.10        23.10
        65536          640        60.64        60.65        60.64
       262144          160       216.85       216.88       216.87
      1048576           40       930.60       932.25       931.43
      4194304           10      5976.51      6042.72      6009.61

#----------------------------------------------------------------
# Benchmarking Alltoall 
# #processes = 4 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
         4096         1000        44.72        44.73        44.73
        16384         1000        68.29        68.31        68.30
        65536          640       173.90       173.94       173.92
       262144          160       598.75       599.08       598.91
      1048576           40      3628.02      3636.63      3632.36
      4194304           10     14537.41     14683.51     14609.86

#----------------------------------------------------------------
# Benchmarking Reduce_scatter 
# #processes = 2 
# ( 2 additional processes waiting in MPI_Barrier)
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
         4096         1000         7.40         7.41         7.41
        16384         1000        25.30        25.30        25.30
        65536          640        68.41        68.42        68.41
       262144          160       278.74       278.83       278.78
      1048576           40       993.47       994.22       993.85
      4194304           10      6586.50      6591.11      6588.80

#----------------------------------------------------------------
# Benchmarking Reduce_scatter 
# #processes = 4 
#----------------------------------------------------------------
       #bytes #repetitions  t_min[usec]  t_max[usec]  t_avg[usec]
         4096         1000        14.68        14.69        14.68
        16384         1000        45.97        45.99        45.98
        65536          640       112.13       112.18       112.16
       262144          160       385.30       385.86       385.54
      1048576           40      1451.13      1452.85      1451.99
      4194304           10      8015.80      8117.51      8065.83


# All processes entering MPI_Finalize
----------------------------------------------------------------
----------------------------------------------------------------
----------------------------------------------------------------
----------------------------------------------------------------
----------------------------------------------------------------
----------------------------------------------------------------
----------------------------------------------------------------
----------------------------------------------------------------
----------------------------------------------------------------
----------------------------------------------------------------
----------------------------------------------------------------
----------------------------------------------------------------
----------------------------------------------------------------
----------------------------------------------------------------
----------------------------------------------------------------