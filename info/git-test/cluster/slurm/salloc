[root@node1 program]# salloc --help
Usage: salloc [OPTIONS...] [executable [args...]]

Parallel run options:
  -A, --account=name          charge job to specified account
      --begin=time            defer job until HH:MM MM/DD/YY
      --bell                  ring the terminal bell when the job is allocated
  -c, --cpus-per-task=ncpus   number of cpus required per task
      --comment=name          arbitrary comment
  -d, --dependency=type:jobid defer job until condition on jobid is satisfied
  -D, --chdir=path            change working directory
      --get-user-env          used by Moab.  See srun man page.
      --gid=group_id          group ID to run job as (user root only)
      --gres=list             required generic resources              ### ###
  -H, --hold                  submit job in held state
  -I, --immediate[=secs]      exit if resources not available in "secs"
      --jobid=id              specify jobid to use
  -J, --job-name=jobname      name of job
  -k, --no-kill               do not kill job on node failure
  -K, --kill-command[=signal] signal to send terminating job
  -L, --licenses=names        required license, comma separated
  -m, --distribution=type     distribution method for processes to nodes  ########
                              (type = block|cyclic|arbitrary)
      --mail-type=type        notify on state change: BEGIN, END, FAIL or ALL
      --mail-user=user        who to send email notification for job state
                              changes
  -n, --tasks=N               number of processors required            #######
      --nice[=value]          decrease scheduling priority by value
      --no-bell               do NOT ring the terminal bell
      --ntasks-per-node=n     number of tasks to invoke on each node    ########  !!!!!!!!!!!
  -N, --nodes=N               number of nodes on which to run (N = min[-max])  #######
  -O, --overcommit            overcommit resources
      --profile=value         enable acct_gather_profile for detailed data
                              value is all or none or any combination of
                              energy, lustre, network or task
  -p, --partition=partition   partition requested
      --qos=qos               quality of service
  -Q, --quiet                 quiet mode (suppress informational messages)
  -s, --share                 share nodes with other jobs   ########
  -t, --time=minutes          time limit
      --time-min=minutes      minimum time limit (if distinct)
      --uid=user_id           user ID to run job as (user root only)
  -v, --verbose               verbose mode (multiple -v's increase verbosity)
      --switches=max-switches{@max-time-to-wait}
                              Optimum switches and max time to wait for optimum

Constraint options:
      --contiguous            demand a contiguous range of nodes     
  -C, --constraint=list       specify a list of constraints
  -F, --nodefile=filename     request a specific list of hosts       ###########  !!!!!!!!!!!!
       --mem=MB                minimum amount of real memory         ########   
      --mincpus=n             minimum number of logical processors (threads) per node  ############ 
      --reservation=name      allocate resources from named reservation                  ##########
      --tmp=MB                minimum amount of temporary disk             ############
  -w, --nodelist=hosts...     request a specific list of hosts            ############## !! !!!!!!!!
  -x, --exclude=hosts...      exclude a specific list of hosts            ############

Consumable resources related options:
      --exclusive             allocate nodes in exclusive mode when
                              cpu consumable resource is enabled
      --mem-per-cpu=MB        maximum amount of real memory per allocated
                              cpu required by the job.
                              --mem >= --mem-per-cpu if --mem is specified.

Affinity/Multi-core options: (when the task/affinity plugin is enabled)
  -B  --extra-node-info=S[:C[:T]]            Expands to:
       --sockets-per-node=S   number of sockets per node to allocate
       --cores-per-socket=C   number of cores per socket to allocate
       --threads-per-core=T   number of threads per core to allocate
                              each field can be 'min' or wildcard '*'
                              total cpus requested = (N x S x C x T)

      --ntasks-per-core=n     number of tasks to invoke on each core         #############
      --ntasks-per-socket=n   number of tasks to invoke on each socket        ##############


Help options:
  -h, --help                  show this help message
  -u, --usage                 display brief usage message

Other options:
  -V, --version               output version information and exit